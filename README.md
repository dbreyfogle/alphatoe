# alphatoe
Reinforcement learning with Tic-Tac-Toe

To run (with Docker):

```
docker build . -t alphatoe
docker run -it alphatoe
```

Sample output:

```
Q(s):
[[(2, 2, 1) 0.131520880859139]
 [(1, 2, 1) 0.12301133800064341]
 [(1, 0, 1) 0.12833792184359577]
 [(1, 1, 1) 0.16165972760661665]
 [(0, 2, 1) 0.1575883128897431]
 [(0, 1, 1) 0.15406692364345934]
 [(0, 0, 1) 0.15778666943807448]
 [(2, 1, 1) 0.1422518964129608]
 [(2, 0, 1) 0.14686645578755594]]

    0   1   2
  *---*---*---*
0 |   |   |   | 
  *---*---*---*
1 |   | X |   | 
  *---*---*---*
2 |   |   |   | 
  *---*---*---*
Row #: 0
Column #: 1

    0   1   2
  *---*---*---*
0 |   | O |   | 
  *---*---*---*
1 |   | X |   | 
  *---*---*---*
2 |   |   |   | 
  *---*---*---*

Q(s):
[[(1, 2, 1) 0.29617267134478]
 [(2, 2, 1) 0.28001912829072934]
 [(1, 0, 1) 0.48681339786017047]
 [(2, 0, 1) 0.34938264513077205]
 [(0, 0, 1) 0.3618090615762824]
 [(0, 2, 1) 0.2862421138051502]
 [(2, 1, 1) 0.14589351446640883]]

    0   1   2
  *---*---*---*
0 |   | O |   | 
  *---*---*---*
1 | X | X |   | 
  *---*---*---*
2 |   |   |   | 
  *---*---*---*
Row #: 1
Column #: 2

    0   1   2
  *---*---*---*
0 |   | O |   | 
  *---*---*---*
1 | X | X | O | 
  *---*---*---*
2 |   |   |   | 
  *---*---*---*

Q(s):
[[(2, 0, 1) 0.4942302564973487]
 [(2, 2, 1) 0.26834692852313236]
 [(0, 2, 1) 0.2623234556078795]
 [(0, 0, 1) 0.49986877463532237]
 [(2, 1, 1) 0.015104822071262656]]

    0   1   2
  *---*---*---*
0 | X | O |   | 
  *---*---*---*
1 | X | X | O | 
  *---*---*---*
2 |   |   |   | 
  *---*---*---*
Row #: 2
Column #: 0

    0   1   2
  *---*---*---*
0 | X | O |   | 
  *---*---*---*
1 | X | X | O | 
  *---*---*---*
2 | O |   |   | 
  *---*---*---*

Q(s):
[[(2, 1, 1) 0.09765100402549906]
 [(0, 2, 1) 0.1452114468946016]
 [(2, 2, 1) 0.9999999999876424]]

    0   1   2
  *---*---*---*
0 | X | O |   | 
  *---*---*---*
1 | X | X | O | 
  *---*---*---*
2 | O |   | X | 
  *---*---*---*
AlphaToe won
Your wins: 0, AlphaToe wins: 1
```
